{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Model Runs notebook\n",
    "\n",
    "Notebook for pre-release QA process, running 2 scenarios from a previous model version and comparing results.\n",
    "\n",
    "‚ö†Ô∏è TODO: Set the paths to the two results files from Azure that you want to run, in the variable \"results_dict\", together with the scheme code. Example given in the first cell.\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Downloads results files from Azure Results container\n",
    "2. Converts the params in the results files into model params .json files, makes minor edits, starts containers with the modified json files on dev\n",
    "3. Checks the status of the runs\n",
    "4. When the runs are completed, compares the dev model run results to the results downloaded from Azure.\n",
    "\n",
    "‚ö†Ô∏è Note that this notebook will only work if there have not been breaking changes in the params files between the model versions being tested. If there have been breaking changes, you will need to add these into the cell where the parameters are edited.\n",
    "\n",
    "The notebook produces and displays dataframes comparing results from the previous model version with the dev version of the model. You will have to use your own eyes üëÄ to check for differences and record any findings/observations on the relevant Prerelease QA checklist GitHub issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Select appropriate scenarios from previous model version\n",
    "# You may have to run one with params-sample from previous model version if this does not already exist\n",
    "\n",
    "results_dict = {\n",
    "    # 1 \"real\" scenario from previous model version\n",
    "    \"RX1\": {\n",
    "        \"results_path\": \"prod/vX.X/RX1/scenarioname.json.gz\"\n",
    "    },\n",
    "    # 1 scenario using sample params from previous model version\n",
    "    \"RX2\": {\n",
    "        \"results_path\": \"prod/vX.X/RX2/scenarioname.json.gz\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params from Azure\n",
    "%cd ../..\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nhpy import az, process_params, process_results\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "load_dotenv()\n",
    "account_url = os.getenv(\"AZ_STORAGE_EP\")\n",
    "results_container = os.getenv(\"AZ_STORAGE_RESULTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get parameters from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenarios that have been run, where results are stored on Azure\n",
    "\n",
    "\n",
    "results_connection = az.connect_to_container(account_url, results_container)\n",
    "\n",
    "\n",
    "for trust in results_dict.keys():\n",
    "\n",
    "    results_path = results_dict[trust][\"results_path\"]\n",
    "    results_json = az.load_results_gzip_file(results_connection, results_path)\n",
    "    results_dict[trust][\"results_old\"] = results_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params only from results JSONs, edit scenario name, save to queue folder\n",
    "# ‚ö†Ô∏è WARNING!!! If there have been any breaking changes to params you will have to implement them here\n",
    "\n",
    "if not os.path.exists(\"queue\"):\n",
    "    os.makedirs(\"queue\")\n",
    "\n",
    "filenames = []\n",
    "for trust in results_dict.keys():\n",
    "    params = results_dict[trust][\"results_old\"][\"params\"].copy()\n",
    "    params[\"scenario\"] = params[\"scenario\"] + \"-testdev\"\n",
    "    params_filename = f\"{params['dataset'] + '-' + params['scenario']}.json\"\n",
    "    params[\"app_version\"] = \"dev\"\n",
    "    params[\"user\"] = \"ds-team\"\n",
    "    params[\"viewable\"] = False\n",
    "    # ‚ö†Ô∏è  FOR v4.2 ONLY: REMOVE LINE BELOW IN NEXT MODEL VERSION TESTING\n",
    "    params[\"inequalities\"] = {\"level_up\": [], \"zero_sum\": [],\"level_down\": []}\n",
    "    with open(os.path.join(\"queue\", params_filename), \"w\") as f:\n",
    "        json.dump(params, f)\n",
    "    results_dict[trust][\"new_params\"] = params\n",
    "    filenames.append(params_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start new model runs on dev using nhp_aci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nhp.aci.run_model import create_model_run\n",
    "from nhp.aci.run_model.helpers import validate_params\n",
    "from nhpy.run_full_results import _track_container_status\n",
    "from nhpy.utils import _construct_results_path\n",
    "\n",
    "responses = []\n",
    "for f in filenames:\n",
    "    with open(os.path.join(\"queue\", f), \"rb\") as fopen:\n",
    "        params = json.load(fopen)\n",
    "        validate_params(params, params[\"app_version\"])\n",
    "        metadata = create_model_run(params, params[\"app_version\"])\n",
    "        # Use container creation datetime\n",
    "        params[\"original_datetime\"] = params[\"create_datetime\"]\n",
    "        params[\"create_datetime\"] = (\n",
    "            metadata[\"create_datetime\"] if metadata else \"\"\n",
    "            )\n",
    "        responses.append(metadata)\n",
    "        results_dict[params[\"dataset\"]][\"new_results_path\"] = _construct_results_path(params)[\"json_path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è  Need to refactor this to automatically poll every 120 seconds show info in the same way as run_full_results\n",
    "# ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è  in the meantime just run this cell every now and then till you see your runs have completed\n",
    "\n",
    "from pprint import pprint\n",
    "from nhp.aci.status.list_current_model_runs import get_current_model_runs\n",
    "\n",
    "current_runs = get_current_model_runs()\n",
    "\n",
    "pprint(current_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for runs to be completed ‚åö\n",
    "\n",
    "This normally takes about 15 mins. Your runs should say \"detail_status\": \"Completed\" (and then disappear from the list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use completed dev run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new model runs from Azure and store in the results_dict\n",
    "\n",
    "for trust in results_dict.keys():\n",
    "    results_path = results_dict[trust][\"new_results_path\"]\n",
    "    results_json = az.load_results_gzip_file(results_connection, results_path)\n",
    "    results_dict[trust][\"results_new\"] = results_json\n",
    "    print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and save to CSV\n",
    "from datetime import date\n",
    "\n",
    "trusts = list(results_dict)\n",
    "df_list = [process_results.compare_results(results_dict, t) for t in trusts]\n",
    "(\n",
    "\n",
    "    pd.concat(df_list)\n",
    "    .reset_index()\n",
    "    .groupby([\"trust\", \"pod\", \"measure\"])\n",
    "    .sum()\n",
    "    .to_csv(f\"QA_default_results_{date.today()}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and save to CSV\n",
    "\n",
    "\n",
    "\n",
    "sc_list = [process_results.compare_stepcounts(results_dict, t) for t in trusts]\n",
    "(\n",
    "    pd.concat(sc_list)\n",
    "    .reset_index()\n",
    "    .fillna(\"-\")\n",
    "    .groupby([\"trust\", \"change_factor\", \"measure\", \"strategy\"])\n",
    "    .sum(numeric_only=True)\n",
    "    .to_csv(f\"QA_stepcounts_{date.today()}.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nhp-products",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
