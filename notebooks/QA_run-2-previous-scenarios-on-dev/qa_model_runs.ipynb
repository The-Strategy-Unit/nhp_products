{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Model Runs notebook\n",
    "\n",
    "Notebook for pre-release QA process, running 2 scenarios from a previous model version and comparing results.\n",
    "\n",
    "‚ö†Ô∏è TODO: Set the paths to the two results files from Azure that you want to run, in the variable \"results_dict\", together with the scheme code. Example given in the first cell.\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Downloads results files from Azure Results container\n",
    "2. Converts the params in the results files into model params .json files, makes minor edits, sends to the API to run on the dev version of the model\n",
    "3. Checks the status of the runs using the API\n",
    "4. When the runs are completed, compares the dev model run results to the results downloaded from Azure.\n",
    "\n",
    "‚ö†Ô∏è Note that this notebook will only work if there have not been breaking changes in the params files between the model versions being tested. If there have been breaking changes, you will need to add these into the cell where the parameters are edited.\n",
    "\n",
    "The notebook produces and displays dataframes comparing results from the previous model version with the dev version of the model. You will have to use your own eyes üëÄ to check for differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run at least 2 scenarios from current users/schemes.\n",
    "# Pick scenarios that are the most recent model version - you may have to run these yourself if they do not already exist.\n",
    "# Also run at least 1 scenario using sample_params and a non-NHP scheme (these test all params, not just the ones set by schemes)\n",
    "\n",
    "results_dict = {\n",
    "    \"RXX\": {\n",
    "        \"results_path\": \"prod/vX.X/RXX/scenarioname-datetime.json.gz\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params from Azure\n",
    "%cd ../..\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nhpy import az, process_params, process_results\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load all environment variables\n",
    "load_dotenv()\n",
    "account_url = os.getenv(\"AZ_STORAGE_EP\")\n",
    "results_container = os.getenv(\"AZ_STORAGE_RESULTS\")\n",
    "api_url = os.getenv(\"API_URL\")\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get parameters from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenarios that have been run, where results are stored on Azure\n",
    "\n",
    "results_connection = az.connect_to_container(account_url, results_container)\n",
    "\n",
    "\n",
    "for trust in results_dict.keys():\n",
    "\n",
    "    results_path = results_dict[trust][\"results_path\"]\n",
    "    results_json = az.load_results_gzip_file(results_connection, results_path)\n",
    "\n",
    "    results_dict[trust][\"results_old\"] = results_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params only from results JSONs, edit scenario name, save to queue folder\n",
    "# ‚ö†Ô∏è For v3.3 there is a breaking change to params - we need to change the format of NDG\n",
    "\n",
    "if not os.path.exists(\"queue\"):\n",
    "    os.makedirs(\"queue\")\n",
    "\n",
    "filenames = []\n",
    "for trust in results_dict.keys():\n",
    "    params = results_dict[trust][\"results_old\"][\"params\"].copy()\n",
    "    params[\"scenario\"] = params[\"scenario\"] + \"-test\"\n",
    "    params_filename = f\"{params['dataset'] + '-' + params['scenario']}.json\"\n",
    "    params[\"app_version\"] = \"dev\"\n",
    "    params[\"user\"] = \"ds-team\"\n",
    "    params[\"viewable\"] = False\n",
    "    with open(os.path.join(\"queue\", params_filename), \"w\") as f:\n",
    "        json.dump(params, f)\n",
    "    results_dict[trust][\"new_params\"] = params\n",
    "    filenames.append(params_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send runs to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "responses = {}\n",
    "for f in filenames:\n",
    "    with open(os.path.join(\"queue\", f), \"rb\") as fopen:\n",
    "        params = json.load(fopen)\n",
    "        response = requests.post(\n",
    "            url=api_url,\n",
    "            params={\n",
    "                \"app_version\": \"dev\",\n",
    "                \"code\": api_key,\n",
    "                \"save_full_model_results\": \"False\",\n",
    "            },\n",
    "            json=params,\n",
    "            timeout=30,\n",
    "        )\n",
    "    time.sleep(3)\n",
    "    responses[params[\"dataset\"]] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "for provider, response in responses.items():\n",
    "    create_datetime = literal_eval(response._content.decode(\"utf-8\"))[\"create_datetime\"]\n",
    "    params = results_dict[provider][\"new_params\"]\n",
    "    results_dict[provider][\n",
    "        \"new_results_path\"\n",
    "    ] = f\"prod/{params['app_version']}/{params['dataset']}/{params['scenario']}-{create_datetime}.json.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for runs to be completed ‚åö\n",
    "\n",
    "At the moment I don't know how to query the API to check if the runs are completed. In the meantime you can check it manually by visiting the URL below in your browser...\n",
    "\n",
    "This normally takes about 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{os.getenv('API_CHECKPOINT')}?code={api_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use completed dev run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new model runs from Azure and store in the results_dict\n",
    "\n",
    "for trust in results_dict.keys():\n",
    "    results_path = results_dict[trust][\"new_results_path\"]\n",
    "    results_json = az.load_results_gzip_file(results_connection, results_path)\n",
    "    results_dict[trust][\"results_new\"] = results_json\n",
    "    print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusts = list(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and save to CSV\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "df_list = [process_results.compare_results(results_dict, t) for t in trusts]\n",
    "(\n",
    "\n",
    "    pd.concat(df_list)\n",
    "    .reset_index()\n",
    "    .groupby([\"trust\", \"pod\", \"measure\"])\n",
    "    .sum()\n",
    "    .to_csv(f\"QA_default_results_{date.today()}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and save to CSV\n",
    "\n",
    "\n",
    "\n",
    "sc_list = [process_results.compare_stepcounts(results_dict, t) for t in trusts]\n",
    "(\n",
    "    pd.concat(sc_list)\n",
    "    .reset_index()\n",
    "    .fillna(\"-\")\n",
    "    .groupby([\"trust\", \"change_factor\", \"measure\", \"strategy\"])\n",
    "    .sum(numeric_only=True)\n",
    "    .to_csv(f\"QA_stepcounts_{date.today()}.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nhp_products",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
