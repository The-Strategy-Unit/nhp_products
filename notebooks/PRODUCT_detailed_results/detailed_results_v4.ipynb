{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed results\n",
    "\n",
    "This notebook is for producing detailed results for model v4.0.0.\n",
    "\n",
    "Assumes you have already authenticated via Azure CLI - [instructions here](https://github.com/The-Strategy-Unit/data_science/blob/fa37cbc01513127626364049124d71f06a35183a/blogs/posts/2024-05-22_storing-data-safely/azure_python.ipynb#L43-L47). Outputs into a `data/` folder the detailed aggregations of IP, OP, and AAE model results in CSV and Parquet formats.\n",
    "\n",
    "Also assumes the scenario has already been run with `full_model_results = True`.\n",
    "\n",
    "You can check if this has happened using `nhpy.check_full_results`, and if not, produce full model results using `nhpy.run_full_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Set this to the path where the aggregated model results are saved\n",
    "\n",
    "agg_results_folder = \"aggregated-model-results/vX.X/RXX/scenarioname/datetime/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to be in the nhp_products root folder so that we can load nhpy.az\n",
    "%cd ../..\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from nhpy import az, process_data, process_results\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "load_dotenv()\n",
    "account_url = os.getenv(\"AZ_STORAGE_EP\")\n",
    "results_container = os.getenv(\"AZ_STORAGE_RESULTS\")\n",
    "data_container = os.getenv(\"AZ_STORAGE_DATA\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "results_connection = az.connect_to_container(account_url, results_container)\n",
    "data_connection = az.connect_to_container(account_url, data_container)\n",
    "params = az.load_agg_params(results_connection, agg_results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lots of info from the results file\n",
    "\n",
    "scenario_name = params[\"scenario\"]\n",
    "trust = params[\"dataset\"]\n",
    "model_version = params[\"app_version\"]\n",
    "baseline_year = params[\"start_year\"]\n",
    "run_id = params[\"create_datetime\"]\n",
    "\n",
    "# Patch model version for loading the data\n",
    "# Results folder name truncated, e.g. v3.0 - does not show the patch version. But data stores in format v3.0.1\n",
    "model_version_data = az.find_latest_version(data_connection, params[\"app_version\"])\n",
    "print(f\"Using data: {model_version_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Data folder if it doesn't exist\n",
    "\n",
    "if not os.path.exists(\"notebooks/PRODUCT_detailed_results/data/\"):\n",
    "    os.makedirs(\"notebooks/PRODUCT_detailed_results/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add principal to the \"vanilla\" model results\n",
    "actual_results_df = az.load_agg_results(results_connection, agg_results_folder)\n",
    "actual_results_df = process_results.convert_results_format(actual_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = az.load_data_file(\n",
    "    data_connection, model_version_data, trust, \"ip\", baseline_year\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = {}\n",
    "for run in range(1, 257):\n",
    "    df = az.load_model_run_results_file(\n",
    "        results_connection, model_version, trust, scenario_name, run_id, \"ip\", run\n",
    "    )\n",
    "    # We want to use the speldur and classpat from the results, not from the original df\n",
    "    merged = (\n",
    "        original_df.copy()\n",
    "        .drop(columns=[\"speldur\", \"classpat\"])\n",
    "        .merge(df, on=\"rn\", how=\"inner\")\n",
    "    )\n",
    "    results = process_data.process_ip_detailed_results(merged)\n",
    "    results_dict = results.to_dict()\n",
    "    for k, v in results_dict[\"value\"].items():\n",
    "        if k not in model_runs.keys():\n",
    "            model_runs[k] = []\n",
    "        model_runs[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs_df = process_data.process_model_runs_dict(\n",
    "    model_runs,\n",
    "    columns=[\n",
    "        \"sitetret\",\n",
    "        \"age_group\",\n",
    "        \"sex\",\n",
    "        \"pod\",\n",
    "        \"tretspef\",\n",
    "        \"los_group\",\n",
    "        \"maternity_delivery_in_spell\",\n",
    "        \"measure\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for checking if \"main\" model results from Azure line up with aggregated model results\n",
    "# Not always the same because of rounding\n",
    "\n",
    "default_beddays_principal = (\n",
    "    actual_results_df[actual_results_df[\"measure\"] == \"beddays\"][\"mean\"]\n",
    "    .sum()\n",
    "    .astype(int)\n",
    ")\n",
    "detailed_beddays_principal = (\n",
    "    model_runs_df.loc[\n",
    "        (\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            \"beddays\",\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    .sum()\n",
    "    .loc[\"mean\"]\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "try:\n",
    "    assert abs(default_beddays_principal - detailed_beddays_principal) <= 1\n",
    "except:\n",
    "    print(default_beddays_principal)\n",
    "    print(detailed_beddays_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "model_runs_df.to_csv(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_ip_results.csv\"\n",
    ")\n",
    "model_runs_df.to_parquet(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_ip_results.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outpatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = az.load_data_file(\n",
    "    data_connection, model_version_data, trust, \"op\", baseline_year\n",
    ")\n",
    "original_df = original_df.rename(columns={\"index\": \"rn\"}).fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_model_runs = {}\n",
    "\n",
    "for run in range(1, 257):\n",
    "    df = az.load_model_run_results_file(\n",
    "        results_connection, model_version, trust, scenario_name, run_id, \"op\", run\n",
    "    )\n",
    "    assert df.shape[0] == original_df.shape[0]\n",
    "    merged = (\n",
    "        original_df.copy()\n",
    "        .drop(columns=[\"attendances\", \"tele_attendances\"])\n",
    "        .merge(df, on=\"rn\", how=\"inner\")\n",
    "    )\n",
    "    results = process_data.process_op_detailed_results(merged)\n",
    "    # Handle activity converted from IP to OP\n",
    "    df_conv = az.load_model_run_results_file(\n",
    "        results_connection,\n",
    "        model_version,\n",
    "        trust,\n",
    "        scenario_name,\n",
    "        run_id,\n",
    "        \"op_conversion\",\n",
    "        run,\n",
    "    )\n",
    "    df_conv = process_data.process_op_converted_from_ip(df_conv)\n",
    "    results = process_data.combine_converted_with_main_results(df_conv, results)\n",
    "    results_dict = results.to_dict()\n",
    "    for k, v in results_dict[\"value\"].items():\n",
    "        if k not in op_model_runs.keys():\n",
    "            op_model_runs[k] = []\n",
    "        op_model_runs[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_model_runs_df = process_data.process_model_runs_dict(\n",
    "    op_model_runs, columns=[\"sitetret\", \"pod\", \"age_group\", \"tretspef\", \"measure\"]\n",
    ")\n",
    "op_model_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for checking if \"main\" model results from Azure line up with aggregated model results using \"full model results\"\n",
    "detailed_attendances_principal = (\n",
    "    op_model_runs_df.round(1)\n",
    "    .loc[(slice(None), slice(None), slice(None), slice(None), \"attendances\"), :]\n",
    "    .sum()\n",
    "    .astype(int)\n",
    "    .loc[\"mean\"]\n",
    ")\n",
    "default_attendances_principal = (\n",
    "    actual_results_df[actual_results_df[\"measure\"] == \"attendances\"][\"mean\"]\n",
    "    .sum()\n",
    "    .astype(int)\n",
    ")\n",
    "# They're not always exactly the same because of rounding\n",
    "try:\n",
    "    assert abs(default_attendances_principal - detailed_attendances_principal) <= 1\n",
    "except:\n",
    "    print(default_attendances_principal)\n",
    "    print(detailed_attendances_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_model_runs_df.to_csv(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_op_results.csv\"\n",
    ")\n",
    "op_model_runs_df.to_parquet(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_op_results.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = az.load_data_file(\n",
    "    data_connection, model_version_data, trust, \"aae\", baseline_year\n",
    ")\n",
    "original_df = original_df.rename(columns={\"index\": \"rn\"}).fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_runs = {}\n",
    "\n",
    "for run in range(1, 257):\n",
    "    df = az.load_model_run_results_file(\n",
    "        results_connection, model_version, trust, scenario_name, run_id, \"aae\", run\n",
    "    )\n",
    "    assert len(df) == len(original_df)\n",
    "    merged = original_df.drop(columns=[\"arrivals\"]).merge(df, on=\"rn\", how=\"inner\")\n",
    "    results = process_data.process_aae_results(merged)\n",
    "    # Handle activity converted from IP to OP\n",
    "    df_conv = az.load_model_run_results_file(\n",
    "        results_connection,\n",
    "        model_version,\n",
    "        trust,\n",
    "        scenario_name,\n",
    "        run_id,\n",
    "        \"sdec_conversion\",\n",
    "        run,\n",
    "    )\n",
    "    df_conv = process_data.process_aae_converted_from_ip(df_conv)\n",
    "    results = process_data.combine_converted_with_main_results(df_conv, results)\n",
    "    results_dict = results.to_dict()\n",
    "    for k, v in results_dict[\"arrivals\"].items():\n",
    "        if k not in ae_model_runs.keys():\n",
    "            ae_model_runs[k] = []\n",
    "        ae_model_runs[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_runs_df = process_data.process_model_runs_dict(\n",
    "    ae_model_runs,\n",
    "    columns=[\n",
    "        \"sitetret\",\n",
    "        \"pod\",\n",
    "        \"age_group\",\n",
    "        \"attendance_category\",\n",
    "        \"aedepttype\",\n",
    "        \"acuity\",\n",
    "        \"measure\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for checking if \"main\" model results from Azure line up with aggregated model results using full model results\n",
    "detailed_ambulance_principal = (\n",
    "    ae_model_runs_df.loc[\n",
    "        (\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            \"ambulance\",\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    .sum()\n",
    "    .loc[\"mean\"]\n",
    "    .round(0)\n",
    ")\n",
    "default_ambulance_principal = (\n",
    "    actual_results_df[actual_results_df[\"measure\"] == \"ambulance\"][\"mean\"]\n",
    "    .sum()\n",
    "    .round(0)\n",
    ")\n",
    "\n",
    "# They're not always exactly the same because of rounding\n",
    "try:\n",
    "    assert abs(default_ambulance_principal - detailed_ambulance_principal) <= 1\n",
    "except:\n",
    "    print(\"OH NO!!\")\n",
    "    print(default_ambulance_principal)\n",
    "    print(detailed_ambulance_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for checking if \"main\" model results from Azure line up with aggregated model results using full model results\n",
    "detailed_walkins_principal = (\n",
    "    ae_model_runs_df.loc[\n",
    "        (\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            \"walk-in\",\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    .sum()\n",
    "    .loc[\"mean\"]\n",
    "    .round(0)\n",
    ")\n",
    "default_walkins_principal = (\n",
    "    actual_results_df[actual_results_df[\"measure\"] == \"walk-in\"][\"mean\"]\n",
    "    .sum()\n",
    "    .round(0)\n",
    ")\n",
    "\n",
    "# They're not always exactly the same because of rounding\n",
    "try:\n",
    "    assert abs(default_walkins_principal - detailed_walkins_principal) <= 1\n",
    "except:\n",
    "    print(\"OH NO!!\")\n",
    "    print(default_walkins_principal)\n",
    "    print(detailed_walkins_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "ae_model_runs_df.to_csv(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_ae_results.csv\"\n",
    ")\n",
    "ae_model_runs_df.to_parquet(\n",
    "    f\"notebooks/PRODUCT_detailed_results/data/{scenario_name}_detailed_ae_results.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nhp_products",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
