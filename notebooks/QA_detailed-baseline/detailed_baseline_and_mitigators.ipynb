{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed baseline including mitigators\n",
    "\n",
    "This is an updated version of the site level mitigators file. It produces 7 CSVs for each trust:\n",
    "- detailed ip baseline\n",
    "- detailed op baseline\n",
    "- detailed aae baseline\n",
    "- ip_activity_avoidance\n",
    "- ip_efficiencies\n",
    "- op_activity_avoidance\n",
    "- aae_activity_avoidance\n",
    "\n",
    "The only thing you need to set is the model_version in the first cell. The data is read directly from Azure and loops through all the trusts currently in the NHP programme. The trusts and the baseline year they are using are listed in the `trusts` variable.\n",
    "\n",
    "⚠️ this notebook only works for v3.7 and later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'v4.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "# Load all environment variables\n",
    "load_dotenv()\n",
    "account_url = os.getenv('AZ_STORAGE_EP')\n",
    "container_name_data = os.getenv('AZ_STORAGE_CONTAINER')\n",
    "container_name_inputs = os.getenv('AZ_STORAGE_INPUTS')\n",
    "# Authenticate\n",
    "default_credential = DefaultAzureCredential()\n",
    "# Connect to containers\n",
    "container_client_data = ContainerClient(account_url, container_name_data, default_credential)\n",
    "container_client_inputs = ContainerClient(account_url, container_name_inputs, default_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusts = [\n",
    "    \"RCF\",\n",
    "    \"RDU\",\n",
    "    \"RGN\",\n",
    "    \"RGP\",\n",
    "    \"RCX\",\n",
    "    \"RBT\",\n",
    "    \"RN5\",\n",
    "    \"RAS\",\n",
    "    \"RQW\",\n",
    "    \"RWG\",\n",
    "    \"R1H\",\n",
    "    \"RWE\",\n",
    "    \"RVR\",\n",
    "    \"RNQ\",\n",
    "    \"RH5\",\n",
    "    \"RA9\",\n",
    "    \"R0A\",\n",
    "    \"RXC\",\n",
    "    \"RTX\",\n",
    "    \"RH8\",\n",
    "    \"RHW\",\n",
    "    \"RXN\",\n",
    "    \"RYJ\",\n",
    "    \"RX1\",\n",
    "    \"RGR\",\n",
    "    \"RD8\",\n",
    "    \"R0A\",\n",
    "    \"REF\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patchless model-version string\n",
    "def simplify_version(model):\n",
    "    v_str = 'dev'\n",
    "    if model != \"dev\":\n",
    "        v_split = model.split(\".\")\n",
    "        v_str = f'{v_split[0]}.{v_split[1]}'  # e.g. v3.0.0 is now v3.0\n",
    "    return v_str\n",
    "\n",
    "# Build string to the write directory\n",
    "def enstring_dir(model, trust, year):\n",
    "    v_dir = simplify_version(model)\n",
    "    dir = f'{v_dir}/{trust}/baseline_data_detailed/{year}'  # mimics Azure path\n",
    "    return dir\n",
    "\n",
    "# Create the write directory if it doesn't already exist\n",
    "def create_dir(model, trust, year):\n",
    "    dir = enstring_dir(model, trust, year)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# Prepare partial filepath for saving output CSVs\n",
    "def create_path_stub(model, trust, year):\n",
    "    dir = enstring_dir(model, trust, year)\n",
    "    path_stub = f'{dir}/{trust}_{model}_{year}'  # later appended with *.csv\n",
    "    return path_stub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pod_to_data_v3(model_results):\n",
    "    # Add beddays\n",
    "    model_results['beddays'] = model_results['speldur'] + 1\n",
    "    # Add pod - this is from _add_pod_to_data() function\n",
    "    model_results[\"pod\"] = \"ip_\" + model_results[\"group\"] + \"_admission\"\n",
    "    classpat = model_results[\"classpat\"]\n",
    "    model_results.loc[classpat == \"2\", \"pod\"] = \"ip_elective_daycase\"\n",
    "    model_results.loc[classpat == \"3\", \"pod\"] = \"ip_regular_day_attender\"\n",
    "    model_results.loc[classpat == \"4\", \"pod\"] = \"ip_regular_night_attender\"\n",
    "    return model_results\n",
    "\n",
    "def get_data_v3(version, trust, activity_type, year=2023):\n",
    "    blob_name = [b for b in container_client_data.list_blob_names(name_starts_with=f'{version}/{activity_type}/fyear={year}/dataset={trust}') if b.endswith('.parquet')][0]\n",
    "    blob_client = container_client_data.get_blob_client(blob_name)\n",
    "    download_stream = blob_client.download_blob()\n",
    "    stream_object = io.BytesIO(download_stream.readall())\n",
    "    data = pd.read_parquet(stream_object)\n",
    "    return data\n",
    "\n",
    "def aggregate_data(data):\n",
    "    data = data.groupby(['sitetret', 'group', 'pod', 'tretspef']).agg({'rn': 'count', 'beddays': 'sum', 'speldur': 'sum'}).rename(columns={'rn': 'admissions'})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trusts:\n",
    "    y = 2023\n",
    "\n",
    "    create_dir(model_version, t, y)  # prep write directory\n",
    "    path_stub = create_path_stub(model_version, t, y)  # prep partial filepath\n",
    "\n",
    "    # get main ip data file and do basic processing\n",
    "    data = get_data_v3(model_version, t, 'ip', y)\n",
    "    data = add_pod_to_data_v3(data)\n",
    "    data['beddays'] = data['speldur'] + 1\n",
    "\n",
    "    # efficiencies\n",
    "    ip_eff = get_data_v3(model_version,t,'ip_efficiencies_strategies', y)\n",
    "    ip_eff = ip_eff.merge(data, how='inner', left_on='rn', right_on='rn')\n",
    "    ip_eff = ip_eff.groupby(['strategy', 'sitetret']).agg({'speldur':'sum', 'beddays':'sum','rn': 'count'}).round(2).rename(columns={'rn': 'admissions'})\n",
    "    ip_eff.to_csv(f'{path_stub}_ip_efficiencies.csv')\n",
    "\n",
    "    # activity avoidance\n",
    "    ip_aa = get_data_v3(model_version,t,'ip_activity_avoidance_strategies',y)\n",
    "    ip_aa = ip_aa.merge(data, how='inner', left_on='rn', right_on='rn')\n",
    "    ip_aa = ip_aa.groupby(['strategy', 'sitetret']).agg(\n",
    "                {'rn': 'count', 'sample_rate': 'sum', 'beddays': 'sum'}).rename(columns={'sample_rate': 'sample_rate_sum', 'rn': 'admissions'})\n",
    "    ip_aa.to_csv(f'{path_stub}_ip_activity_avoidance.csv')\n",
    "\n",
    "    # aggregate and save main ip data file\n",
    "    data = aggregate_data(data)\n",
    "    data.to_csv(f'{path_stub}_detailed_ip_baseline.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outpatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cons_cons(df):\n",
    "    cons_df = df[df['is_cons_cons_ref']][[\"type\", 'sitetret', 'attendances', 'tele_attendances']]\n",
    "    cons_df = cons_df.groupby(['sitetret', 'type']).sum()\n",
    "    return cons_df\n",
    "\n",
    "## Followup reduction\n",
    "# NOT FIRST, NO PROCEDURES\n",
    "def get_followup_df(df):\n",
    "    followup_df = df[~df['has_procedures']]\n",
    "    followup_df = followup_df[~followup_df['is_first']][['sitetret', 'type', 'attendances', 'tele_attendances']]\n",
    "    followup_df = followup_df.groupby(['sitetret', 'type']).sum()\n",
    "    return followup_df\n",
    "\n",
    "# ## GP referred first attendance\n",
    "# # IS FIRST AND IS GP REFERRED\n",
    "def get_gp_df(df):\n",
    "    gp_df = df[df['is_gp_ref']]\n",
    "    gp_df = gp_df[gp_df['is_first']][['sitetret', 'type', 'attendances', 'tele_attendances']]\n",
    "    gp_df = gp_df.groupby(['sitetret', 'type']).sum()\n",
    "    return gp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trusts:\n",
    "    op_data = get_data_v3(model_version, t,'op', y)\n",
    "    tele_df = op_data[~op_data['has_procedures']].copy()\n",
    "    tele_df = tele_df.groupby(['sitetret', \"type\"])[[\"attendances\",\"tele_attendances\"]].sum()\n",
    "    tele_df = tele_df.rename(columns = {'attendances': 'convert_to_tele: attendances', 'tele_attendances': 'convert_to_tele: tele_attendances'})\n",
    "    op_aa = tele_df.copy()\n",
    "    cons_df = get_cons_cons(op_data).rename(columns={'attendances': 'consultant_to_consultant_referrals_attendances',\n",
    "                                                    'tele_attendances': 'consultant_to_consultant_referrals_tele_attendances'})\n",
    "    op_aa = op_aa.merge(cons_df, left_index = True, right_index = True, how='outer')\n",
    "    followup_df = get_followup_df(op_data).rename(columns = {'attendances': 'followup_reduction_attendances',\n",
    "                                                            'tele_attendances': 'followup_reduction_tele_attendances'})\n",
    "    op_aa = op_aa.merge(followup_df, left_index = True, right_index = True, how = 'outer')\n",
    "    gp_df = get_gp_df(op_data).rename(columns = {'attendances': 'gp_referred_attendances',\n",
    "                                                'tele_attendances': 'gp_referred_tele_attendances'})\n",
    "    op_aa = op_aa.merge(gp_df, left_index = True, right_index = True, how = 'outer')\n",
    "    op_aa = op_aa.fillna(0).astype(int)\n",
    "\n",
    "    create_dir(model_version, t, y)  # prep write directory\n",
    "    path_stub = create_path_stub(model_version, t, y)  # prep partial filepath\n",
    "    op_aa.to_csv(f'{path_stub}_op_activity_avoidance.csv')\n",
    "    op_data.groupby(['sitetret', 'group', 'tretspef', 'type']).agg({'attendances': 'sum', 'tele_attendances': 'sum'}).to_csv(f'{path_stub}_detailed_op_baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ae_aggregation(df, col_name):\n",
    "    df = df[df[col_name]].groupby(['sitetret', 'hsagrp'])[['arrivals']].sum()\n",
    "    df = df.rename(columns={'arrivals': col_name})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trusts:\n",
    "        ae_data = get_data_v3(model_version, t,'aae', y)\n",
    "        # start with empty df\n",
    "        ae_df = pd.DataFrame(index=ae_data.groupby(['sitetret', 'hsagrp']).count().index)\n",
    "\n",
    "        for col in ['is_discharged_no_treatment', 'is_frequent_attender', 'is_left_before_treatment',\n",
    "                'is_low_cost_referred_or_discharged']:\n",
    "                df = get_ae_aggregation(ae_data, col)\n",
    "                ae_df = ae_df.merge(df, left_index=True, right_index=True, how=\"outer\").fillna(0)\n",
    "\n",
    "        create_dir(model_version, t, y)  # prep write directory\n",
    "        path_stub = create_path_stub(model_version, t, y)  # prep partial filepath\n",
    "\n",
    "        ae_df.to_csv(f'{path_stub}_aae_activity_avoidance.csv')\n",
    "        ae_data.groupby(['sitetret', 'hsagrp', 'aedepttype', 'attendance_category'])[['arrivals']].sum().to_csv(f'{path_stub}_detailed_aae_baseline.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data to Azure\n",
    "\n",
    "CSVs were generated above in the local `vX.Y/XYZ/baseline_data_detailed/YYYY/` directory structure. They'll be copied to the same directory structure on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trusts:\n",
    "\n",
    "    path = enstring_dir(model_version, t, y)\n",
    "    filepaths = f'{path}/*.csv'\n",
    "    for filepath in glob.glob(filepaths, recursive = True):\n",
    "        with open(file = filepath, mode = \"rb\") as data:\n",
    "            try:\n",
    "                blob_client = container_client_inputs.upload_blob(name = filepath, data = data, overwrite=True)\n",
    "            except:\n",
    "                print(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nhp_products",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
